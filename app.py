# ==================== ç¯å¢ƒé…ç½® ====================
import os
os.environ["XFORMERS_DISABLED"] = "1"  # ç¦ç”¨xformersçš„Tritonä¾èµ–ï¼Œè§£å†³Windowså…¼å®¹é—®é¢˜

# ==================== åº“å¯¼å…¥ ====================
import torch
import einops
import numpy as np
import gradio as gr
from PIL import Image, ImageDraw
from PIL.Image import Resampling
import matplotlib.pyplot as plt
from matplotlib.cm import get_cmap
from ultralytics import YOLO  # å¯¼å…¥YOLOç±»
import cv2
import tempfile

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from depthfm import DepthFM

# ==================== å·¥å…·å‡½æ•° ====================
def get_dtype_from_str(dtype_str):
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¯¹åº”çš„torchæ•°æ®ç±»å‹"""
    dtype_map = {
        "fp32": torch.float32,
        "fp16": torch.float16, 
        "bf16": torch.bfloat16
    }
    return dtype_map[dtype_str]

def resize_max_res(img, max_edge_resolution, resample=Resampling.BILINEAR):
    """
    ä¿æŒå®½é«˜æ¯”è°ƒæ•´å›¾åƒå°ºå¯¸ï¼Œç¡®ä¿é•¿è¾¹ä¸è¶…è¿‡æŒ‡å®šåˆ†è¾¨ç‡
    å‚æ•°ï¼š
        img: PILå›¾åƒå¯¹è±¡
        max_edge_resolution: æœ€å¤§è¾¹é•¿ï¼ˆåƒç´ ï¼‰
        resample: é‡é‡‡æ ·æ–¹æ³•
    è¿”å›ï¼š
        è°ƒæ•´åçš„å›¾åƒå’ŒåŸå§‹å°ºå¯¸å…ƒç»„
    """
    original_w, original_h = img.size
    scale = min(max_edge_resolution/original_w, max_edge_resolution/original_h)
    
    new_w = int(original_w * scale)
    new_h = int(original_h * scale)
    new_w = (new_w // 64) * 64  # ç¡®ä¿å°ºå¯¸æ˜¯64çš„å€æ•°
    new_h = (new_h // 64) * 64
    
    return img.resize((new_w, new_h), resample=resample), (original_w, original_h)

def load_im(input_image, processing_res=-1):
    """
    å›¾åƒé¢„å¤„ç†ç®¡é“
    å‚æ•°ï¼š
        input_image: Gradioä¼ å…¥çš„numpyæ•°ç»„æ ¼å¼å›¾åƒ
        processing_res: å¤„ç†åˆ†è¾¨ç‡
    è¿”å›ï¼š
        é¢„å¤„ç†åçš„å¼ é‡å’ŒåŸå§‹å°ºå¯¸
    """
    # è½¬æ¢è¾“å…¥æ ¼å¼
    pil_img = Image.fromarray(input_image).convert('RGB')
    
    # è‡ªåŠ¨ç¡®å®šå¤„ç†åˆ†è¾¨ç‡
    if processing_res < 0:
        processing_res = max(pil_img.size)
        
    # è°ƒæ•´å°ºå¯¸
    resized_img, orig_size = resize_max_res(pil_img, processing_res)
    
    # å½’ä¸€åŒ–å¤„ç†
    img_array = np.array(resized_img)
    img_tensor = einops.rearrange(img_array, 'h w c -> c h w')  # è°ƒæ•´ç»´åº¦é¡ºåº
    img_tensor = img_tensor / 127.5 - 1  # å½’ä¸€åŒ–åˆ°[-1, 1]
    img_tensor = torch.tensor(img_tensor, dtype=torch.float32)[None]  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
    
    return img_tensor, orig_size

# ==================== æ¨¡å‹åˆå§‹åŒ– ====================
# ä½¿ç”¨GPUå¦‚æœå¯ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# åˆå§‹åŒ–æ¨¡å‹
depth_model = DepthFM("checkpoints/depthfm-v1.ckpt")
depth_model = depth_model.to(device).eval()

# åˆå§‹åŒ–YOLOv11æ¨¡å‹
yolo_model = YOLO("checkpoints/yolo11n.pt")  # ä½¿ç”¨ultralyticsçš„YOLOç±»

# ==================== å¤„ç†å‡½æ•° ====================
def process_image(input_img, 
                depth_enabled=True, 
                yolo_enabled=True,
                num_steps=2, 
                ensemble_size=4, 
                processing_res=-1, 
                depth_colormap="magma",
                confidence_threshold=0.5):
    """
    å›¾åƒå¤„ç†ä¸»å‡½æ•°
    å‚æ•°:
        input_img: è¾“å…¥å›¾åƒ (numpyæ•°ç»„)
        depth_enabled: æ˜¯å¦å¯ç”¨æ·±åº¦ä¼°è®¡
        yolo_enabled: æ˜¯å¦å¯ç”¨ç›®æ ‡æ£€æµ‹
        num_steps: DepthFMçš„ODEæ±‚è§£æ­¥æ•°
        ensemble_size: DepthFMçš„é›†æˆå¤§å°
        processing_res: å¤„ç†åˆ†è¾¨ç‡
        depth_colormap: æ·±åº¦å›¾é¢œè‰²æ˜ å°„åç§°
        confidence_threshold: ç›®æ ‡æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
    è¿”å›:
        åŸå§‹å›¾åƒ, æ·±åº¦å›¾, ç›®æ ‡æ£€æµ‹ç»“æœ, èåˆç»“æœ
    """
    if input_img is None:
        return None, None, None, None
    
    # è½¬æ¢ä¸ºPILå›¾åƒå¤‡ç”¨
    if isinstance(input_img, np.ndarray):
        pil_img = Image.fromarray(input_img)
    else:
        pil_img = input_img
        input_img = np.array(pil_img)
    
    # ç»“æœåˆå§‹åŒ–
    depth_result = None
    detection_result = None
    fusion_result = None
    
    # -------- æ·±åº¦ä¼°è®¡ --------
    if depth_enabled:
        # æ•°æ®é¢„å¤„ç†
        input_tensor, orig_size = load_im(input_img, processing_res)
        input_tensor = input_tensor.to(device)
        
        # æ·±åº¦ä¼°è®¡
        with torch.autocast(device_type="cuda", dtype=torch.float16):
            depth_map = depth_model.predict_depth(
                input_tensor,
                num_steps=num_steps,
                ensemble_size=ensemble_size
            )
        
        # åå¤„ç†
        depth_np = depth_map.squeeze().cpu().numpy()
        result = plt.get_cmap(depth_colormap)(depth_np, bytes=True)[..., :3]
        
        # æ¢å¤åŸå§‹å°ºå¯¸
        depth_result = Image.fromarray(result)
        if depth_result.size != orig_size:
            depth_result = depth_result.resize(orig_size, Resampling.BILINEAR)
    
    # -------- ç›®æ ‡æ£€æµ‹ --------
    detections = []
    if yolo_enabled:
        # ä½¿ç”¨ultralyticsçš„YOLOè¿›è¡Œæ£€æµ‹
        results = yolo_model(input_img, conf=confidence_threshold)
        
        # æå–æ£€æµ‹ç»“æœ
        for result in results:
            boxes = result.boxes
            for box in boxes:
                # è·å–è¾¹ç•Œæ¡†åæ ‡
                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                conf = box.conf[0].cpu().numpy()
                cls_id = int(box.cls[0].cpu().numpy())
                cls_name = yolo_model.names[cls_id]
                
                detections.append({
                    'box': [int(x1), int(y1), int(x2), int(y2)],
                    'confidence': float(conf),
                    'class_id': cls_id,
                    'class_name': cls_name
                })
        
        print(f"æ£€æµ‹åˆ° {len(detections)} ä¸ªå¯¹è±¡: {detections}")  # è°ƒè¯•è¾“å‡º
        
        # å¯è§†åŒ–æ£€æµ‹ç»“æœ
        detection_result = results[0].plot()  # ä½¿ç”¨ultralyticsçš„å†…ç½®å¯è§†åŒ–
        detection_result = Image.fromarray(detection_result)
    
    # -------- èåˆç»“æœ --------
    if depth_enabled and yolo_enabled and len(detections) > 0:
        # å‡†å¤‡èåˆå›¾åƒåŸºäºæ·±åº¦å›¾
        fusion_img = depth_result.copy() if depth_result else pil_img.copy()
        
        # åœ¨èåˆå›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ
        draw = ImageDraw.Draw(fusion_img)
        
        # æå–æ·±åº¦ä¿¡æ¯å’Œç›®æ ‡æ£€æµ‹ä¿¡æ¯
        for det in detections:
            box = det['box']
            cls_name = det['class_name']
            conf = det['confidence']
            
            # ä½¿ç”¨ç±»åˆ«åç§°ä½œä¸ºé¢œè‰²é”®
            color = (255, 0, 0)  # é»˜è®¤çº¢è‰²
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            draw.rectangle(box, outline=color, width=3)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{cls_name} {conf:.2f}"
            draw.text((box[0], box[1]-20), label, fill=(255, 255, 255))
            
            # è®¡ç®—è¾¹ç•Œæ¡†ä¸­å¿ƒç‚¹çš„æ·±åº¦å€¼ï¼ˆå¦‚æœæœ‰æ·±åº¦å›¾ï¼‰
            if depth_result:
                center_x = (box[0] + box[2]) // 2
                center_y = (box[1] + box[3]) // 2
                
                # ç»˜åˆ¶ä¸­å¿ƒç‚¹
                draw.ellipse([center_x-5, center_y-5, center_x+5, center_y+5], fill=(255, 0, 0))
        
        fusion_result = fusion_img
    
    # å¦‚æœåªå¯ç”¨äº†ä¸€ä¸ªåŠŸèƒ½ï¼Œä½¿ç”¨è¯¥åŠŸèƒ½çš„ç»“æœä½œä¸ºèåˆç»“æœ
    if fusion_result is None:
        if depth_enabled and depth_result is not None:
            fusion_result = depth_result
        elif yolo_enabled and detection_result is not None:
            fusion_result = detection_result
        else:
            fusion_result = pil_img
    
    return pil_img, depth_result, detection_result, fusion_result

def process_video(input_video, num_steps=2, ensemble_size=4, processing_res=512, depth_colormap="magma", confidence_threshold=0.5, progress=gr.Progress()):
    """
    è§†é¢‘å¤„ç†ä¸»å‡½æ•°
    å‚æ•°:
        input_video: è¾“å…¥è§†é¢‘è·¯å¾„
        num_steps: DepthFMçš„ODEæ±‚è§£æ­¥æ•°
        ensemble_size: DepthFMçš„é›†æˆå¤§å°
        processing_res: å¤„ç†åˆ†è¾¨ç‡
        depth_colormap: æ·±åº¦å›¾é¢œè‰²æ˜ å°„åç§°
        confidence_threshold: ç›®æ ‡æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
        progress: Gradioè¿›åº¦æ¡å¯¹è±¡
    è¿”å›:
        å¤„ç†åçš„è§†é¢‘è·¯å¾„
    """
    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(input_video)
    if not cap.isOpened():
        raise ValueError("æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
    
    # è·å–è§†é¢‘ä¿¡æ¯
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # åˆ›å»ºä¸´æ—¶è¾“å‡ºæ–‡ä»¶
    output_path = tempfile.NamedTemporaryFile(suffix='.mp4', delete=False).name
    
    # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    # è®¾ç½®è¿›åº¦æ¡
    progress(0, desc="åˆå§‹åŒ–...")
    
    try:
        frame_idx = 0
        processed_idx = 0
        
        # é€å¸§å¤„ç†
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
                
            # è¿›åº¦æ›´æ–°
            progress(processed_idx/frame_count, desc=f"å¤„ç†å¸§ {processed_idx+1}/{frame_count}")
            
            # è½¬æ¢ä¸ºRGBï¼ˆOpenCVä½¿ç”¨BGRï¼‰
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # æ·±åº¦ä¼°è®¡
            depth_result = process_image(
                frame_rgb, 
                depth_enabled=True,
                yolo_enabled=True,
                num_steps=num_steps,
                ensemble_size=ensemble_size,
                processing_res=processing_res,
                depth_colormap=depth_colormap,
                confidence_threshold=confidence_threshold
            )
            
            # è½¬æ¢å›OpenCVæ ¼å¼
            depth_frame = np.array(depth_result[3])  # èåˆç»“æœ
            depth_frame_bgr = cv2.cvtColor(depth_frame, cv2.COLOR_RGB2BGR)
            
            # å†™å…¥è¾“å‡ºè§†é¢‘
            out.write(depth_frame_bgr)
            processed_idx += 1
            
            # æ¯100å¸§æ£€æŸ¥æ˜¯å¦ç”¨æˆ·å–æ¶ˆ
            if frame_idx % 100 == 0 and progress.is_cancelled():
                break
                
    except Exception as e:
        raise RuntimeError(f"è§†é¢‘å¤„ç†å‡ºé”™: {str(e)}")
    finally:
        # é‡Šæ”¾èµ„æº
        cap.release()
        out.release()
        progress(1.0, desc="å¤„ç†å®Œæˆ")
        
    return output_path

# ==================== Gradioç•Œé¢è®¾è®¡ ====================
# ç¤ºä¾‹å›¾ç‰‡é…ç½®
EXAMPLE_DIR = "examples"  # ç¤ºä¾‹æ–‡ä»¶ç›®å½•
demo_samples = [
    [os.path.join(EXAMPLE_DIR, "img/dog.png"), True, True, 2, 4, -1, "magma", 0.5],
]

# é¢œè‰²æ˜ å°„é€‰é¡¹
COLORMAP_OPTIONS = ["magma", "viridis", "inferno", "plasma", "cividis", "turbo", "jet"]

with gr.Blocks(title="3Dæ„ŸçŸ¥ä¸ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# å•ç›®è§†è§‰3Dæ„ŸçŸ¥ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ")
    gr.Markdown("åŸºäºDepthFMå’ŒYOLOv11çš„3Dæ„ŸçŸ¥æ£€æµ‹ç³»ç»Ÿ")
    
    with gr.Row():
        # å·¦ä¾§æ§åˆ¶é¢æ¿
        with gr.Column(scale=1):
            input_type = gr.Radio(
                choices=["å›¾åƒ", "è§†é¢‘"],
                value="å›¾åƒ",
                label="è¾“å…¥ç±»å‹",
                info="é€‰æ‹©å¤„ç†çš„åª’ä½“ç±»å‹"
            )
            
            img_input = gr.Image(label="è¾“å…¥å›¾åƒ", type="numpy", visible=True)
            video_input = gr.Video(label="è¾“å…¥è§†é¢‘", visible=False)
            
            with gr.Row():
                depth_enabled = gr.Checkbox(label="å¯ç”¨æ·±åº¦ä¼°è®¡", value=True)
                yolo_enabled = gr.Checkbox(label="å¯ç”¨ç›®æ ‡æ£€æµ‹", value=True)
            
            with gr.Accordion("æ·±åº¦ä¼°è®¡å‚æ•°", open=False):
                num_steps = gr.Slider(
                    minimum=1, maximum=10, value=2,
                    label="ODEæ±‚è§£æ­¥æ•°", step=1
                )
                ensemble_size = gr.Slider(
                    minimum=1, maximum=10, value=4,
                    label="é›†æˆæ¬¡æ•°", step=1
                )
                processing_res = gr.Slider(
                    minimum=64, maximum=2048, value=-1,
                    label="å¤„ç†åˆ†è¾¨ç‡ï¼ˆ-1=è‡ªåŠ¨ï¼‰", step=64
                )
                depth_colormap = gr.Dropdown(
                    COLORMAP_OPTIONS, value="magma", 
                    label="æ·±åº¦å›¾é¢œè‰²æ˜ å°„"
                )
            
            with gr.Accordion("ç›®æ ‡æ£€æµ‹å‚æ•°", open=False):
                confidence_threshold = gr.Slider(
                    minimum=0.1, maximum=1.0, value=0.5,
                    label="ç½®ä¿¡åº¦é˜ˆå€¼", step=0.05
                )
            
            submit_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary", visible=True)
            video_submit_btn = gr.Button("ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘", variant="primary", visible=False)
        
        # å³ä¾§ç»“æœå±•ç¤º
        with gr.Column(scale=2):
            with gr.Tab("èåˆç»“æœ"):
                fusion_output = gr.Image(label="èåˆç»“æœ", type="pil", visible=True)
                video_output = gr.Video(label="è§†é¢‘è¾“å‡º", visible=False)
            with gr.Tab("å•ç‹¬ç»“æœ"):
                with gr.Row():
                    original_output = gr.Image(label="åŸå§‹å›¾åƒ", type="pil")
                    depth_output = gr.Image(label="æ·±åº¦ä¼°è®¡", type="pil")
                with gr.Row():
                    detection_output = gr.Image(label="ç›®æ ‡æ£€æµ‹", type="pil")
    
    # è”åŠ¨å‡½æ•°
    def switch_input(input_type):
        if input_type == "å›¾åƒ":
            return gr.update(visible=True), gr.update(visible=False), gr.update(visible=True), gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)
        else:
            return gr.update(visible=False), gr.update(visible=True), gr.update(visible=False), gr.update(visible=True), gr.update(visible=False), gr.update(visible=True)
    
    input_type.change(
        switch_input,
        inputs=[input_type],
        outputs=[img_input, video_input, submit_btn, video_submit_btn, fusion_output, video_output]
    )

    # ç¤ºä¾‹åŒºå—
    gr.Examples(
        examples=demo_samples,
        inputs=[img_input, depth_enabled, yolo_enabled, num_steps, 
                ensemble_size, processing_res, depth_colormap, confidence_threshold],
        outputs=[original_output, depth_output, detection_output, fusion_output],
        fn=process_image,
        cache_examples=False,  # ç¦ç”¨ç¼“å­˜é¿å…è·¯å¾„é—®é¢˜
        label="ç¤ºä¾‹å›¾ç‰‡"
    )

    # æŒ‰é’®äº‹ä»¶ç»‘å®š
    submit_btn.click(
        fn=process_image,
        inputs=[img_input, depth_enabled, yolo_enabled, num_steps, 
                ensemble_size, processing_res, depth_colormap, confidence_threshold],
        outputs=[original_output, depth_output, detection_output, fusion_output]
    )
    
    video_submit_btn.click(
        fn=process_video,
        inputs=[video_input, num_steps, ensemble_size, processing_res, depth_colormap, confidence_threshold],
        outputs=video_output
    )

# ==================== å¯åŠ¨åº”ç”¨ ====================
if __name__ == "__main__":
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=True,  # ç”Ÿæˆå…¬å…±è®¿é—®é“¾æ¥
        allowed_paths=[os.path.abspath(EXAMPLE_DIR)]  # å…è®¸è®¿é—®ç¤ºä¾‹ç›®å½•
    )