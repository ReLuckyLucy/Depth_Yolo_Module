# ==================== ç¯å¢ƒé…ç½® ====================
import os
os.environ["XFORMERS_DISABLED"] = "1"  # ç¦ç”¨xformersçš„Tritonä¾èµ–ï¼Œè§£å†³Windowså…¼å®¹é—®é¢˜

# ==================== åº“å¯¼å…¥ ====================
import torch
import einops
import numpy as np
import gradio as gr
from PIL import Image, ImageDraw
from PIL.Image import Resampling
import matplotlib.pyplot as plt
from matplotlib.cm import get_cmap

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from depthfm import DepthFM
from yolov11 import YOLOv11

# ==================== å·¥å…·å‡½æ•° ====================
def get_dtype_from_str(dtype_str):
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¯¹åº”çš„torchæ•°æ®ç±»å‹"""
    dtype_map = {
        "fp32": torch.float32,
        "fp16": torch.float16, 
        "bf16": torch.bfloat16
    }
    return dtype_map[dtype_str]

def resize_max_res(img, max_edge_resolution, resample=Resampling.BILINEAR):
    """
    ä¿æŒå®½é«˜æ¯”è°ƒæ•´å›¾åƒå°ºå¯¸ï¼Œç¡®ä¿é•¿è¾¹ä¸è¶…è¿‡æŒ‡å®šåˆ†è¾¨ç‡
    å‚æ•°ï¼š
        img: PILå›¾åƒå¯¹è±¡
        max_edge_resolution: æœ€å¤§è¾¹é•¿ï¼ˆåƒç´ ï¼‰
        resample: é‡é‡‡æ ·æ–¹æ³•
    è¿”å›ï¼š
        è°ƒæ•´åçš„å›¾åƒå’ŒåŸå§‹å°ºå¯¸å…ƒç»„
    """
    original_w, original_h = img.size
    scale = min(max_edge_resolution/original_w, max_edge_resolution/original_h)
    
    new_w = int(original_w * scale)
    new_h = int(original_h * scale)
    new_w = (new_w // 64) * 64  # ç¡®ä¿å°ºå¯¸æ˜¯64çš„å€æ•°
    new_h = (new_h // 64) * 64
    
    return img.resize((new_w, new_h), resample=resample), (original_w, original_h)

def load_im(input_image, processing_res=-1):
    """
    å›¾åƒé¢„å¤„ç†ç®¡é“
    å‚æ•°ï¼š
        input_image: Gradioä¼ å…¥çš„numpyæ•°ç»„æ ¼å¼å›¾åƒ
        processing_res: å¤„ç†åˆ†è¾¨ç‡
    è¿”å›ï¼š
        é¢„å¤„ç†åçš„å¼ é‡å’ŒåŸå§‹å°ºå¯¸
    """
    # è½¬æ¢è¾“å…¥æ ¼å¼
    pil_img = Image.fromarray(input_image).convert('RGB')
    
    # è‡ªåŠ¨ç¡®å®šå¤„ç†åˆ†è¾¨ç‡
    if processing_res < 0:
        processing_res = max(pil_img.size)
        
    # è°ƒæ•´å°ºå¯¸
    resized_img, orig_size = resize_max_res(pil_img, processing_res)
    
    # å½’ä¸€åŒ–å¤„ç†
    img_array = np.array(resized_img)
    img_tensor = einops.rearrange(img_array, 'h w c -> c h w')  # è°ƒæ•´ç»´åº¦é¡ºåº
    img_tensor = img_tensor / 127.5 - 1  # å½’ä¸€åŒ–åˆ°[-1, 1]
    img_tensor = torch.tensor(img_tensor, dtype=torch.float32)[None]  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
    
    return img_tensor, orig_size

# ==================== æ¨¡å‹åˆå§‹åŒ– ====================
# ä½¿ç”¨GPUå¦‚æœå¯ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# åˆå§‹åŒ–æ¨¡å‹
depth_model = DepthFM("checkpoints/depthfm-v1.ckpt")
depth_model = depth_model.to(device).eval()
yolo_model = YOLOv11("checkpoints/yolo11n.pt", device)

# ==================== å¤„ç†å‡½æ•° ====================
def process_image(input_img, 
                depth_enabled=True, 
                yolo_enabled=True,
                num_steps=2, 
                ensemble_size=4, 
                processing_res=-1, 
                depth_colormap="magma",
                confidence_threshold=0.5):
    """
    å›¾åƒå¤„ç†ä¸»å‡½æ•°
    å‚æ•°:
        input_img: è¾“å…¥å›¾åƒ (numpyæ•°ç»„)
        depth_enabled: æ˜¯å¦å¯ç”¨æ·±åº¦ä¼°è®¡
        yolo_enabled: æ˜¯å¦å¯ç”¨ç›®æ ‡æ£€æµ‹
        num_steps: DepthFMçš„ODEæ±‚è§£æ­¥æ•°
        ensemble_size: DepthFMçš„é›†æˆå¤§å°
        processing_res: å¤„ç†åˆ†è¾¨ç‡
        depth_colormap: æ·±åº¦å›¾é¢œè‰²æ˜ å°„åç§°
        confidence_threshold: ç›®æ ‡æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
    è¿”å›:
        åŸå§‹å›¾åƒ, æ·±åº¦å›¾, ç›®æ ‡æ£€æµ‹ç»“æœ, èåˆç»“æœ
    """
    if input_img is None:
        return None, None, None, None
    
    # è½¬æ¢ä¸ºPILå›¾åƒå¤‡ç”¨
    if isinstance(input_img, np.ndarray):
        pil_img = Image.fromarray(input_img)
    else:
        pil_img = input_img
        input_img = np.array(pil_img)
    
    # ç»“æœåˆå§‹åŒ–
    depth_result = None
    detection_result = None
    fusion_result = None
    
    # -------- æ·±åº¦ä¼°è®¡ --------
    if depth_enabled:
        # æ•°æ®é¢„å¤„ç†
        input_tensor, orig_size = load_im(input_img, processing_res)
        input_tensor = input_tensor.to(device)
        
        # æ·±åº¦ä¼°è®¡
        with torch.autocast(device_type="cuda", dtype=torch.float16):
            depth_map = depth_model.predict_depth(
                input_tensor,
                num_steps=num_steps,
                ensemble_size=ensemble_size
            )
        
        # åå¤„ç†
        depth_np = depth_map.squeeze().cpu().numpy()
        result = plt.get_cmap(depth_colormap)(depth_np, bytes=True)[..., :3]
        
        # æ¢å¤åŸå§‹å°ºå¯¸
        depth_result = Image.fromarray(result)
        if depth_result.size != orig_size:
            depth_result = depth_result.resize(orig_size, Resampling.BILINEAR)
    
    # -------- ç›®æ ‡æ£€æµ‹ --------
    detections = []
    if yolo_enabled:
        detections = yolo_model.detect(input_img)
        print(f"æ£€æµ‹åˆ° {len(detections)} ä¸ªå¯¹è±¡: {detections}")  # è°ƒè¯•è¾“å‡º
        
        # è¿‡æ»¤ä½ç½®ä¿¡åº¦æ£€æµ‹
        detections = [d for d in detections if d['confidence'] >= confidence_threshold]
        
        # å¯è§†åŒ–æ£€æµ‹ç»“æœ
        detection_result = yolo_model.visualize(pil_img, detections)
    
    # -------- èåˆç»“æœ --------
    if depth_enabled and yolo_enabled and len(detections) > 0:
        # å‡†å¤‡èåˆå›¾åƒåŸºäºæ·±åº¦å›¾
        fusion_img = depth_result.copy() if depth_result else pil_img.copy()
        
        # åœ¨èåˆå›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ
        draw = ImageDraw.Draw(fusion_img)
        
        # æå–æ·±åº¦ä¿¡æ¯å’Œç›®æ ‡æ£€æµ‹ä¿¡æ¯
        for det in detections:
            box = det['box']
            cls_name = det['class_name']
            conf = det['confidence']
            color = yolo_model.colors.get(cls_name, (255, 255, 255))
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            draw.rectangle(box, outline=color, width=3)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{cls_name} {conf:.2f}"
            draw.text((box[0], box[1]-20), label, fill=(255, 255, 255))
            
            # è®¡ç®—è¾¹ç•Œæ¡†ä¸­å¿ƒç‚¹çš„æ·±åº¦å€¼ï¼ˆå¦‚æœæœ‰æ·±åº¦å›¾ï¼‰
            if depth_result:
                center_x = (box[0] + box[2]) // 2
                center_y = (box[1] + box[3]) // 2
                
                # ç»˜åˆ¶ä¸­å¿ƒç‚¹
                draw.ellipse([center_x-5, center_y-5, center_x+5, center_y+5], fill=(255, 0, 0))
        
        fusion_result = fusion_img
    
    # å¦‚æœåªå¯ç”¨äº†ä¸€ä¸ªåŠŸèƒ½ï¼Œä½¿ç”¨è¯¥åŠŸèƒ½çš„ç»“æœä½œä¸ºèåˆç»“æœ
    if fusion_result is None:
        if depth_enabled and depth_result is not None:
            fusion_result = depth_result
        elif yolo_enabled and detection_result is not None:
            fusion_result = detection_result
        else:
            fusion_result = pil_img
    
    return pil_img, depth_result, detection_result, fusion_result

# ==================== Gradioç•Œé¢è®¾è®¡ ====================
# ç¤ºä¾‹å›¾ç‰‡é…ç½®
EXAMPLE_DIR = "examples"  # ç¤ºä¾‹æ–‡ä»¶ç›®å½•
demo_samples = [
    [os.path.join(EXAMPLE_DIR, "img/bus.jpg"), True, True, 2, 4, -1, "magma", 0.5],
]

# é¢œè‰²æ˜ å°„é€‰é¡¹
COLORMAP_OPTIONS = ["magma", "viridis", "inferno", "plasma", "cividis", "turbo", "jet"]

with gr.Blocks(title="3Dæ„ŸçŸ¥ä¸ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    gr.Markdown("#å•ç›®è§†è§‰3Dæ„ŸçŸ¥ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ")
    gr.Markdown("åŸºäºDepthFMå’ŒYOLOv11çš„3Dæ„ŸçŸ¥æ£€æµ‹ç³»ç»Ÿ")
    
    with gr.Row():
        # å·¦ä¾§æ§åˆ¶é¢æ¿
        with gr.Column(scale=1):
            img_input = gr.Image(label="è¾“å…¥å›¾åƒ", type="numpy")
            
            with gr.Row():
                depth_enabled = gr.Checkbox(label="å¯ç”¨æ·±åº¦ä¼°è®¡", value=True)
                yolo_enabled = gr.Checkbox(label="å¯ç”¨ç›®æ ‡æ£€æµ‹", value=True)
            
            with gr.Accordion("æ·±åº¦ä¼°è®¡å‚æ•°", open=False):
                num_steps = gr.Slider(
                    minimum=1, maximum=10, value=2,
                    label="ODEæ±‚è§£æ­¥æ•°", step=1
                )
                ensemble_size = gr.Slider(
                    minimum=1, maximum=10, value=4,
                    label="é›†æˆæ¬¡æ•°", step=1
                )
                processing_res = gr.Slider(
                    minimum=64, maximum=2048, value=-1,
                    label="å¤„ç†åˆ†è¾¨ç‡ï¼ˆ-1=è‡ªåŠ¨ï¼‰", step=64
                )
                depth_colormap = gr.Dropdown(
                    COLORMAP_OPTIONS, value="magma", 
                    label="æ·±åº¦å›¾é¢œè‰²æ˜ å°„"
                )
            
            with gr.Accordion("ç›®æ ‡æ£€æµ‹å‚æ•°", open=False):
                confidence_threshold = gr.Slider(
                    minimum=0.1, maximum=1.0, value=0.5,
                    label="ç½®ä¿¡åº¦é˜ˆå€¼", step=0.05
                )
            
            submit_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
        
        # å³ä¾§ç»“æœå±•ç¤º
        with gr.Column(scale=2):
            with gr.Tab("èåˆç»“æœ"):
                fusion_output = gr.Image(label="èåˆç»“æœ", type="pil")
            
            with gr.Tab("å•ç‹¬ç»“æœ"):
                with gr.Row():
                    original_output = gr.Image(label="åŸå§‹å›¾åƒ", type="pil")
                    depth_output = gr.Image(label="æ·±åº¦ä¼°è®¡", type="pil")
                
                with gr.Row():
                    detection_output = gr.Image(label="ç›®æ ‡æ£€æµ‹", type="pil")
    
    # ç¤ºä¾‹åŒºå—
    gr.Examples(
        examples=demo_samples,
        inputs=[img_input, depth_enabled, yolo_enabled, num_steps, 
                ensemble_size, processing_res, depth_colormap, confidence_threshold],
        outputs=[original_output, depth_output, detection_output, fusion_output],
        fn=process_image,
        cache_examples=False,  # ç¦ç”¨ç¼“å­˜é¿å…è·¯å¾„é—®é¢˜
        label="ç¤ºä¾‹å›¾ç‰‡"
    )

    # æŒ‰é’®äº‹ä»¶ç»‘å®š
    submit_btn.click(
        fn=process_image,
        inputs=[img_input, depth_enabled, yolo_enabled, num_steps, 
                ensemble_size, processing_res, depth_colormap, confidence_threshold],
        outputs=[original_output, depth_output, detection_output, fusion_output]
    )

# ==================== å¯åŠ¨åº”ç”¨ ====================
if __name__ == "__main__":
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=True,  # ç”Ÿæˆå…¬å…±è®¿é—®é“¾æ¥
        allowed_paths=[os.path.abspath(EXAMPLE_DIR)]  # å…è®¸è®¿é—®ç¤ºä¾‹ç›®å½•
    )