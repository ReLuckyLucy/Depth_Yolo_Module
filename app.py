# ==================== ç¯å¢ƒé…ç½® ====================
import os
os.environ["XFORMERS_DISABLED"] = "1"  # ç¦ç”¨xformersçš„Tritonä¾èµ–ï¼Œè§£å†³Windowså…¼å®¹é—®é¢˜

# ==================== åº“å¯¼å…¥ ====================
import torch
import numpy as np
import gradio as gr
from PIL import Image, ImageDraw
import matplotlib.pyplot as plt
from matplotlib.cm import get_cmap

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
import gradio_depth_app  # ä¿®æ”¹ä¸ºå¯¼å…¥æ•´ä¸ªæ¨¡å—
from yolov11 import YOLOv11  # YOLOv11ç›®æ ‡æ£€æµ‹æ¨¡å—

# ==================== æ¨¡å‹åˆå§‹åŒ– ====================
# ä½¿ç”¨GPUå¦‚æœå¯ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# åˆå§‹åŒ–YOLOv11æ¨¡å‹ (DepthFMä¼šåœ¨å¤„ç†å‡½æ•°ä¸­æ‡’åŠ è½½)
yolo_model = YOLOv11("checkpoints/yolo11n.pt", device)

# ==================== å¤„ç†å‡½æ•° ====================
def process_image(input_img, 
                depth_enabled=True, 
                yolo_enabled=True,
                num_steps=2, 
                ensemble_size=4, 
                processing_res=-1, 
                depth_colormap="magma",
                confidence_threshold=0.5):
    """
    å›¾åƒå¤„ç†ä¸»å‡½æ•°
    å‚æ•°:
        input_img: è¾“å…¥å›¾åƒ (numpyæ•°ç»„)
        depth_enabled: æ˜¯å¦å¯ç”¨æ·±åº¦ä¼°è®¡
        yolo_enabled: æ˜¯å¦å¯ç”¨ç›®æ ‡æ£€æµ‹
        num_steps: DepthFMçš„ODEæ±‚è§£æ­¥æ•°
        ensemble_size: DepthFMçš„é›†æˆå¤§å°
        processing_res: å¤„ç†åˆ†è¾¨ç‡
        depth_colormap: æ·±åº¦å›¾é¢œè‰²æ˜ å°„åç§°
        confidence_threshold: ç›®æ ‡æ£€æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
    è¿”å›:
        åŸå§‹å›¾åƒ, æ·±åº¦å›¾, ç›®æ ‡æ£€æµ‹ç»“æœ, èåˆç»“æœ
    """
    if input_img is None:
        return None, None, None, None
    
    # è½¬æ¢ä¸ºPILå›¾åƒå¤‡ç”¨
    if isinstance(input_img, np.ndarray):
        pil_img = Image.fromarray(input_img)
    else:
        pil_img = input_img
        input_img = np.array(pil_img)
    
    # ç»“æœåˆå§‹åŒ–
    depth_result = None
    detection_result = None
    fusion_result = None
    
    # -------- æ·±åº¦ä¼°è®¡ --------
    if depth_enabled:
        # ç›´æ¥è°ƒç”¨gradio_depth_appæ¨¡å—ä¸­çš„process_imageå‡½æ•°
        depth_result = gradio_depth_app.process_image(
            input_img, 
            num_steps=num_steps,
            ensemble_size=ensemble_size,
            processing_res=processing_res,
            no_color=False,  # å§‹ç»ˆä½¿ç”¨å½©è‰²æ·±åº¦å›¾
            dtype="fp16" if torch.cuda.is_available() else "fp32"
        )
    
    # -------- ç›®æ ‡æ£€æµ‹ --------
    detections = []
    if yolo_enabled:
        detections = yolo_model.detect(input_img)
        
        # è¿‡æ»¤ä½ç½®ä¿¡åº¦æ£€æµ‹
        detections = [d for d in detections if d['confidence'] >= confidence_threshold]
        
        # å¯è§†åŒ–æ£€æµ‹ç»“æœ
        detection_result = yolo_model.visualize(pil_img, detections)
    
    # -------- èåˆç»“æœ --------
    if depth_enabled and yolo_enabled and len(detections) > 0:
        # å‡†å¤‡èåˆå›¾åƒåŸºäºæ·±åº¦å›¾
        fusion_img = depth_result.copy() if depth_result else pil_img.copy()
        
        # åœ¨èåˆå›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ
        draw = ImageDraw.Draw(fusion_img)
        
        # æå–æ·±åº¦ä¿¡æ¯å’Œç›®æ ‡æ£€æµ‹ä¿¡æ¯
        for det in detections:
            box = det['box']
            cls_name = det['class_name']
            conf = det['confidence']
            color = yolo_model.colors.get(cls_name, (255, 255, 255))
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            draw.rectangle(box, outline=color, width=3)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{cls_name} {conf:.2f}"
            draw.text((box[0], box[1]-20), label, fill=(255, 255, 255))
            
            # è®¡ç®—è¾¹ç•Œæ¡†ä¸­å¿ƒç‚¹çš„æ·±åº¦å€¼ï¼ˆå¦‚æœæœ‰æ·±åº¦å›¾ï¼‰
            if depth_result:
                center_x = (box[0] + box[2]) // 2
                center_y = (box[1] + box[3]) // 2
                
                # ç»˜åˆ¶ä¸­å¿ƒç‚¹
                draw.ellipse([center_x-5, center_y-5, center_x+5, center_y+5], fill=(255, 0, 0))
        
        fusion_result = fusion_img
    
    # å¦‚æœåªå¯ç”¨äº†ä¸€ä¸ªåŠŸèƒ½ï¼Œä½¿ç”¨è¯¥åŠŸèƒ½çš„ç»“æœä½œä¸ºèåˆç»“æœ
    if fusion_result is None:
        if depth_enabled and depth_result is not None:
            fusion_result = depth_result
        elif yolo_enabled and detection_result is not None:
            fusion_result = detection_result
        else:
            fusion_result = pil_img
    
    return pil_img, depth_result, detection_result, fusion_result

# ==================== Gradioç•Œé¢è®¾è®¡ ====================
# ç¤ºä¾‹å›¾ç‰‡é…ç½®
EXAMPLE_DIR = "examples"  # ç¤ºä¾‹æ–‡ä»¶ç›®å½•
demo_samples = [
    [os.path.join(EXAMPLE_DIR, "img/dog.png"), True, True, 2, 4, -1, "magma", 0.5],
]

# é¢œè‰²æ˜ å°„é€‰é¡¹
COLORMAP_OPTIONS = ["magma", "viridis", "inferno", "plasma", "cividis", "turbo", "jet"]

with gr.Blocks(title="3Dæ„ŸçŸ¥ä¸ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸš— å•ç›®è§†è§‰3Dæ„ŸçŸ¥ç›®æ ‡æ£€æµ‹ç³»ç»Ÿ")
    gr.Markdown("åŸºäºDepthFMå’ŒYOLOv11çš„3Dæ„ŸçŸ¥ä¸è½¦è¾†æ£€æµ‹ç³»ç»Ÿ")
    
    with gr.Row():
        # å·¦ä¾§æ§åˆ¶é¢æ¿
        with gr.Column(scale=1):
            img_input = gr.Image(label="è¾“å…¥å›¾åƒ", type="numpy")
            
            with gr.Row():
                depth_enabled = gr.Checkbox(label="å¯ç”¨æ·±åº¦ä¼°è®¡", value=True)
                yolo_enabled = gr.Checkbox(label="å¯ç”¨ç›®æ ‡æ£€æµ‹", value=True)
            
            with gr.Accordion("æ·±åº¦ä¼°è®¡å‚æ•°", open=False):
                num_steps = gr.Slider(
                    minimum=1, maximum=10, value=2,
                    label="ODEæ±‚è§£æ­¥æ•°", step=1
                )
                ensemble_size = gr.Slider(
                    minimum=1, maximum=10, value=4,
                    label="é›†æˆæ¬¡æ•°", step=1
                )
                processing_res = gr.Slider(
                    minimum=64, maximum=2048, value=-1,
                    label="å¤„ç†åˆ†è¾¨ç‡ï¼ˆ-1=è‡ªåŠ¨ï¼‰", step=64
                )
                depth_colormap = gr.Dropdown(
                    COLORMAP_OPTIONS, value="magma", 
                    label="æ·±åº¦å›¾é¢œè‰²æ˜ å°„"
                )
            
            with gr.Accordion("ç›®æ ‡æ£€æµ‹å‚æ•°", open=False):
                confidence_threshold = gr.Slider(
                    minimum=0.1, maximum=1.0, value=0.5,
                    label="ç½®ä¿¡åº¦é˜ˆå€¼", step=0.05
                )
            
            submit_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
        
        # å³ä¾§ç»“æœå±•ç¤º
        with gr.Column(scale=2):
            with gr.Tab("èåˆç»“æœ"):
                fusion_output = gr.Image(label="èåˆç»“æœ", type="pil")
            
            with gr.Tab("å•ç‹¬ç»“æœ"):
                with gr.Row():
                    original_output = gr.Image(label="åŸå§‹å›¾åƒ", type="pil")
                    depth_output = gr.Image(label="æ·±åº¦ä¼°è®¡", type="pil")
                
                with gr.Row():
                    detection_output = gr.Image(label="ç›®æ ‡æ£€æµ‹", type="pil")
    
    # ç¤ºä¾‹åŒºå—
    gr.Examples(
        examples=demo_samples,
        inputs=[img_input, depth_enabled, yolo_enabled, num_steps, 
                ensemble_size, processing_res, depth_colormap, confidence_threshold],
        outputs=[original_output, depth_output, detection_output, fusion_output],
        fn=process_image,
        cache_examples=False,  # ç¦ç”¨ç¼“å­˜é¿å…è·¯å¾„é—®é¢˜
        label="ç¤ºä¾‹å›¾ç‰‡"
    )

    # æŒ‰é’®äº‹ä»¶ç»‘å®š
    submit_btn.click(
        fn=process_image,
        inputs=[img_input, depth_enabled, yolo_enabled, num_steps, 
                ensemble_size, processing_res, depth_colormap, confidence_threshold],
        outputs=[original_output, depth_output, detection_output, fusion_output]
    )

# ==================== å¯åŠ¨åº”ç”¨ ====================
if __name__ == "__main__":
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=True,  # ç”Ÿæˆå…¬å…±è®¿é—®é“¾æ¥
        allowed_paths=[os.path.abspath(EXAMPLE_DIR)]  # å…è®¸è®¿é—®ç¤ºä¾‹ç›®å½•
    )