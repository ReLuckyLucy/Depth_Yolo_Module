"""
DepthFM Gradioå¯è§†åŒ–ç•Œé¢ - ä¿®å¤ç‰ˆ
ä½œè€…ï¼šAIåŠ©æ‰‹
ç‰ˆæœ¬ï¼š1.1
æœ€åæ›´æ–°ï¼š2024-05-20
"""

# ==================== ç¯å¢ƒé…ç½® ====================
import os
os.environ["XFORMERS_DISABLED"] = "1"  # ç¦ç”¨xformersçš„Tritonä¾èµ–ï¼Œè§£å†³Windowså…¼å®¹é—®é¢˜

# ==================== åº“å¯¼å…¥ ====================
import torch
import einops
import numpy as np
from PIL import Image
from PIL.Image import Resampling
import matplotlib.pyplot as plt
import gradio as gr

# æœ¬åœ°æ¨¡å‹å¯¼å…¥
from depthfm import DepthFM

# ==================== å·¥å…·å‡½æ•° ====================
def get_dtype_from_str(dtype_str):
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¯¹åº”çš„torchæ•°æ®ç±»å‹"""
    dtype_map = {
        "fp32": torch.float32,
        "fp16": torch.float16, 
        "bf16": torch.bfloat16
    }
    return dtype_map[dtype_str]

def resize_max_res(img, max_edge_resolution, resample=Resampling.BILINEAR):
    """
    ä¿æŒå®½é«˜æ¯”è°ƒæ•´å›¾åƒå°ºå¯¸ï¼Œç¡®ä¿é•¿è¾¹ä¸è¶…è¿‡æŒ‡å®šåˆ†è¾¨ç‡
    å‚æ•°ï¼š
        img: PILå›¾åƒå¯¹è±¡
        max_edge_resolution: æœ€å¤§è¾¹é•¿ï¼ˆåƒç´ ï¼‰
        resample: é‡é‡‡æ ·æ–¹æ³•
    è¿”å›ï¼š
        è°ƒæ•´åçš„å›¾åƒå’ŒåŸå§‹å°ºå¯¸å…ƒç»„
    """
    original_w, original_h = img.size
    scale = min(max_edge_resolution/original_w, max_edge_resolution/original_h)
    
    new_w = int(original_w * scale)
    new_h = int(original_h * scale)
    new_w = (new_w // 64) * 64  # ç¡®ä¿å°ºå¯¸æ˜¯64çš„å€æ•°
    new_h = (new_h // 64) * 64
    
    return img.resize((new_w, new_h), resample=resample), (original_w, original_h)

def load_im(input_image, processing_res=-1):
    """
    å›¾åƒé¢„å¤„ç†ç®¡é“
    å‚æ•°ï¼š
        input_image: Gradioä¼ å…¥çš„numpyæ•°ç»„æ ¼å¼å›¾åƒ
        processing_res: å¤„ç†åˆ†è¾¨ç‡
    è¿”å›ï¼š
        é¢„å¤„ç†åçš„å¼ é‡å’ŒåŸå§‹å°ºå¯¸
    """
    # è½¬æ¢è¾“å…¥æ ¼å¼
    pil_img = Image.fromarray(input_image).convert('RGB')
    
    # è‡ªåŠ¨ç¡®å®šå¤„ç†åˆ†è¾¨ç‡
    if processing_res < 0:
        processing_res = max(pil_img.size)
        
    # è°ƒæ•´å°ºå¯¸
    resized_img, orig_size = resize_max_res(pil_img, processing_res)
    
    # å½’ä¸€åŒ–å¤„ç†
    img_array = np.array(resized_img)
    img_tensor = einops.rearrange(img_array, 'h w c -> c h w')  # è°ƒæ•´ç»´åº¦é¡ºåº
    img_tensor = img_tensor / 127.5 - 1  # å½’ä¸€åŒ–åˆ°[-1, 1]
    img_tensor = torch.tensor(img_tensor, dtype=torch.float32)[None]  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
    
    return img_tensor, orig_size

# ==================== æ ¸å¿ƒå¤„ç†å‡½æ•° ====================
def process_image(input_img, num_steps=2, ensemble_size=4, processing_res=-1, no_color=False, dtype="fp16"):
    """
    æ·±åº¦ä¼°è®¡å¤„ç†ä¸»å‡½æ•°
    å‚æ•°ï¼š
        input_img: è¾“å…¥å›¾åƒï¼ˆnumpyæ•°ç»„ï¼‰
        num_steps: ODEæ±‚è§£æ­¥æ•°
        ensemble_size: é›†æˆæ¬¡æ•°
        processing_res: å¤„ç†åˆ†è¾¨ç‡
        no_color: æ˜¯å¦ä½¿ç”¨ç°åº¦è¾“å‡º
        dtype: è®¡ç®—ç²¾åº¦
    è¿”å›ï¼š
        æ·±åº¦å›¾ï¼ˆPILå›¾åƒå¯¹è±¡ï¼‰
    """
    # ---------- æ¨¡å‹åˆå§‹åŒ– ----------
    if not hasattr(process_image, "model"):
        # å•ä¾‹æ¨¡å¼ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        process_image.model = DepthFM("checkpoints/depthfm-v1.ckpt")
        process_image.model = process_image.model.to(device).eval()
        process_image.device = device
    
    # ---------- æ•°æ®é¢„å¤„ç† ----------
    input_tensor, orig_size = load_im(input_img, processing_res)
    input_tensor = input_tensor.to(process_image.device)
    
    # ---------- æ·±åº¦ä¼°è®¡ ----------
    model_dtype = get_dtype_from_str(dtype)
    process_image.model.model.dtype = model_dtype
    
    with torch.autocast(device_type="cuda", dtype=model_dtype):
        depth_map = process_image.model.predict_depth(
            input_tensor,
            num_steps=num_steps,
            ensemble_size=ensemble_size
        )
    
    # ---------- åå¤„ç† ----------
    depth_np = depth_map.squeeze().cpu().numpy()  # ç§»é™¤æ‰¹æ¬¡å’Œé€šé“ç»´åº¦
    
    # é¢œè‰²æ˜ å°„
    if no_color:
        result = (depth_np * 255).astype(np.uint8)
    else:
        result = plt.get_cmap('magma')(depth_np, bytes=True)[..., :3]
    
    # æ¢å¤åŸå§‹å°ºå¯¸
    result_pil = Image.fromarray(result)
    if result_pil.size != orig_size:
        result_pil = result_pil.resize(orig_size, Resampling.BILINEAR)
    
    return result_pil

# ==================== Gradioç•Œé¢é…ç½® ====================
# ç¤ºä¾‹æ–‡ä»¶é…ç½®
EXAMPLE_DIR = "examples"  # ç¤ºä¾‹å›¾ç‰‡å­˜æ”¾ç›®å½•
demo_samples = [
    [os.path.join(EXAMPLE_DIR, "img/dog.png"), 2, 4, -1, False],
]

# ç•Œé¢å¸ƒå±€
with gr.Blocks(title="DepthFM æ·±åº¦ä¼°è®¡æ¼”ç¤º", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¯ DepthFM å•ç›®æ·±åº¦ä¼°è®¡ç³»ç»Ÿ")
    gr.Markdown(" ä¸Šä¼ å›¾ç‰‡ç”Ÿæˆæ·±åº¦å›¾ï¼Œæ”¯æŒå‚æ•°è°ƒèŠ‚å’Œå¤šç§è¾“å‡ºæ ¼å¼")
    
    with gr.Row():
        # å·¦ä¾§æ§åˆ¶é¢æ¿
        with gr.Column(scale=1):
            img_input = gr.Image(label="è¾“å…¥å›¾åƒ", type="numpy")
            
            with gr.Accordion("é«˜çº§å‚æ•°", open=False):
                processing_res = gr.Slider(
                    minimum=64, maximum=2048, value=-1,
                    label="å¤„ç†åˆ†è¾¨ç‡ï¼ˆ-1=è‡ªåŠ¨ï¼‰", step=64
                )
                num_steps = gr.Slider(
                    minimum=1, maximum=10, value=2,
                    label="ODEæ±‚è§£æ­¥æ•°", step=1
                )
                ensemble_size = gr.Slider(
                    minimum=1, maximum=10, value=4,
                    label="é›†æˆæ¬¡æ•°", step=1
                )
                dtype = gr.Dropdown(
                    ["fp16", "fp32", "bf16"], value="fp16",
                    label="è®¡ç®—ç²¾åº¦"
                )
                no_color = gr.Checkbox(
                    label="ç°åº¦è¾“å‡º", info="å‹¾é€‰å¯ç”¨å•é€šé“æ·±åº¦å›¾"
                )
            
            submit_btn = gr.Button("ğŸš€ å¼€å§‹è®¡ç®—", variant="primary")
        
        # å³ä¾§ç»“æœå±•ç¤º
        with gr.Column(scale=2):
            img_output = gr.Image(label="æ·±åº¦å›¾ç»“æœ", type="pil")
    
    # ç¤ºä¾‹åŒºå—
    gr.Examples(
        examples=demo_samples,
        inputs=[img_input, num_steps, ensemble_size, processing_res, no_color],
        outputs=img_output,
        fn=process_image,
        cache_examples=False,  # ç¦ç”¨ç¼“å­˜é¿å…è·¯å¾„é—®é¢˜
        label="å¿«é€Ÿç¤ºä¾‹"
    )

    # æŒ‰é’®äº‹ä»¶ç»‘å®š
    submit_btn.click(
        fn=process_image,
        inputs=[img_input, num_steps, ensemble_size, processing_res, no_color, dtype],
        outputs=img_output
    )

# ==================== å¯åŠ¨åº”ç”¨ ====================
if __name__ == "__main__":
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,  # ç”Ÿæˆå…¬å…±è®¿é—®é“¾æ¥
        allowed_paths=[os.path.abspath(EXAMPLE_DIR)]  # å…è®¸è®¿é—®ç¤ºä¾‹ç›®å½•
    )



