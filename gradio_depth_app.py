# ==================== ç¯å¢ƒé…ç½® ====================
import os
os.environ["XFORMERS_DISABLED"] = "1"  # ç¦ç”¨xformersçš„Tritonä¾èµ–ï¼Œè§£å†³Windowså…¼å®¹é—®é¢˜

# ==================== åº“å¯¼å…¥ ====================
import torch
import einops
import numpy as np
from PIL import Image
from PIL.Image import Resampling
import matplotlib.pyplot as plt
import gradio as gr
import cv2
import tempfile
from tqdm import tqdm
import time

# æœ¬åœ°æ¨¡å‹å¯¼å…¥
from depthfm import DepthFM

# ==================== å·¥å…·å‡½æ•° ====================
def get_dtype_from_str(dtype_str):
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¯¹åº”çš„torchæ•°æ®ç±»å‹"""
    dtype_map = {
        "fp32": torch.float32,
        "fp16": torch.float16, 
        "bf16": torch.bfloat16
    }
    return dtype_map[dtype_str]

def resize_max_res(img, max_edge_resolution, resample=Resampling.BILINEAR):
    """
    ä¿æŒå®½é«˜æ¯”è°ƒæ•´å›¾åƒå°ºå¯¸ï¼Œç¡®ä¿é•¿è¾¹ä¸è¶…è¿‡æŒ‡å®šåˆ†è¾¨ç‡
    å‚æ•°ï¼š
        img: PILå›¾åƒå¯¹è±¡
        max_edge_resolution: æœ€å¤§è¾¹é•¿ï¼ˆåƒç´ ï¼‰
        resample: é‡é‡‡æ ·æ–¹æ³•
    è¿”å›ï¼š
        è°ƒæ•´åçš„å›¾åƒå’ŒåŸå§‹å°ºå¯¸å…ƒç»„
    """
    original_w, original_h = img.size
    scale = min(max_edge_resolution/original_w, max_edge_resolution/original_h)
    
    new_w = int(original_w * scale)
    new_h = int(original_h * scale)
    new_w = (new_w // 64) * 64  # ç¡®ä¿å°ºå¯¸æ˜¯64çš„å€æ•°
    new_h = (new_h // 64) * 64
    
    return img.resize((new_w, new_h), resample=resample), (original_w, original_h)

def load_im(input_image, processing_res=-1):
    """
    å›¾åƒé¢„å¤„ç†ç®¡é“
    å‚æ•°ï¼š
        input_image: Gradioä¼ å…¥çš„numpyæ•°ç»„æ ¼å¼å›¾åƒ
        processing_res: å¤„ç†åˆ†è¾¨ç‡
    è¿”å›ï¼š
        é¢„å¤„ç†åçš„å¼ é‡å’ŒåŸå§‹å°ºå¯¸
    """
    # è½¬æ¢è¾“å…¥æ ¼å¼
    pil_img = Image.fromarray(input_image).convert('RGB')
    
    # è‡ªåŠ¨ç¡®å®šå¤„ç†åˆ†è¾¨ç‡
    if processing_res < 0:
        processing_res = max(pil_img.size)
        
    # è°ƒæ•´å°ºå¯¸
    resized_img, orig_size = resize_max_res(pil_img, processing_res)
    
    # å½’ä¸€åŒ–å¤„ç†
    img_array = np.array(resized_img)
    img_tensor = einops.rearrange(img_array, 'h w c -> c h w')  # è°ƒæ•´ç»´åº¦é¡ºåº
    img_tensor = img_tensor / 127.5 - 1  # å½’ä¸€åŒ–åˆ°[-1, 1]
    img_tensor = torch.tensor(img_tensor, dtype=torch.float32)[None]  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
    
    return img_tensor, orig_size

# ==================== æ ¸å¿ƒå¤„ç†å‡½æ•° ====================
def process_image(input_img, num_steps=2, ensemble_size=4, processing_res=-1, no_color=False, dtype="fp16"):
    """
    æ·±åº¦ä¼°è®¡å¤„ç†ä¸»å‡½æ•°
    å‚æ•°ï¼š
        input_img: è¾“å…¥å›¾åƒï¼ˆnumpyæ•°ç»„ï¼‰
        num_steps: ODEæ±‚è§£æ­¥æ•°
        ensemble_size: é›†æˆæ¬¡æ•°
        processing_res: å¤„ç†åˆ†è¾¨ç‡
        no_color: æ˜¯å¦ä½¿ç”¨ç°åº¦è¾“å‡º
        dtype: è®¡ç®—ç²¾åº¦
    è¿”å›ï¼š
        æ·±åº¦å›¾ï¼ˆPILå›¾åƒå¯¹è±¡ï¼‰
    """
    # ---------- æ¨¡å‹åˆå§‹åŒ– ----------
    if not hasattr(process_image, "model"):
        # å•ä¾‹æ¨¡å¼ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        process_image.model = DepthFM("checkpoints/depthfm-v1.ckpt")
        process_image.model = process_image.model.to(device).eval()
        process_image.device = device
    
    # ---------- æ•°æ®é¢„å¤„ç† ----------
    input_tensor, orig_size = load_im(input_img, processing_res)
    input_tensor = input_tensor.to(process_image.device)
    
    # ---------- æ·±åº¦ä¼°è®¡ ----------
    model_dtype = get_dtype_from_str(dtype)
    process_image.model.model.dtype = model_dtype
    
    with torch.autocast(device_type="cuda", dtype=model_dtype):
        depth_map = process_image.model.predict_depth(
            input_tensor,
            num_steps=num_steps,
            ensemble_size=ensemble_size
        )
    
    # ---------- åå¤„ç† ----------
    depth_np = depth_map.squeeze().cpu().numpy()  # ç§»é™¤æ‰¹æ¬¡å’Œé€šé“ç»´åº¦
    
    # é¢œè‰²æ˜ å°„
    if no_color:
        result = (depth_np * 255).astype(np.uint8)
    else:
        result = plt.get_cmap('magma')(depth_np, bytes=True)[..., :3]
    
    # æ¢å¤åŸå§‹å°ºå¯¸
    result_pil = Image.fromarray(result)
    if result_pil.size != orig_size:
        result_pil = result_pil.resize(orig_size, Resampling.BILINEAR)
    
    return result_pil

# ==================== è§†é¢‘å¤„ç†åŠŸèƒ½ ====================
def process_video(input_video, num_steps=2, ensemble_size=4, processing_res=512, no_color=False, dtype="fp16", frame_stride=1, progress=gr.Progress()):
    """
    è§†é¢‘æ·±åº¦ä¼°è®¡å¤„ç†
    å‚æ•°ï¼š
        input_video: è¾“å…¥è§†é¢‘è·¯å¾„
        num_steps: ODEæ±‚è§£æ­¥æ•°
        ensemble_size: é›†æˆæ¬¡æ•°
        processing_res: å¤„ç†åˆ†è¾¨ç‡
        no_color: æ˜¯å¦ä½¿ç”¨ç°åº¦è¾“å‡º
        dtype: è®¡ç®—ç²¾åº¦
        frame_stride: å¸§é‡‡æ ·æ­¥é•¿ï¼ˆæ¯éš”å¤šå°‘å¸§å¤„ç†ä¸€å¸§ï¼‰
        progress: Gradioè¿›åº¦æ¡å¯¹è±¡
    è¿”å›ï¼š
        å¤„ç†åçš„è§†é¢‘è·¯å¾„
    """
    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(input_video)
    if not cap.isOpened():
        raise ValueError("æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
    
    # è·å–è§†é¢‘ä¿¡æ¯
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # æ ¹æ®å¸§é‡‡æ ·æ­¥é•¿è®¡ç®—å®é™…å¤„ç†çš„å¸§æ•°
    processed_frames = frame_count // frame_stride
    
    # åˆ›å»ºä¸´æ—¶è¾“å‡ºæ–‡ä»¶
    output_path = tempfile.NamedTemporaryFile(suffix='.mp4', delete=False).name
    
    # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps/frame_stride, (width, height))
    
    # è®¾ç½®è¿›åº¦æ¡
    progress(0, desc="åˆå§‹åŒ–...")
    
    try:
        frame_idx = 0
        processed_idx = 0
        
        # é€å¸§å¤„ç†
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
                
            # æŒ‰æ­¥é•¿é‡‡æ ·
            if frame_idx % frame_stride == 0:
                # è¿›åº¦æ›´æ–°
                progress(processed_idx/processed_frames, desc=f"å¤„ç†å¸§ {processed_idx+1}/{processed_frames}")
                
                # è½¬æ¢ä¸ºRGBï¼ˆOpenCVä½¿ç”¨BGRï¼‰
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # æ·±åº¦ä¼°è®¡
                depth_result = process_image(
                    frame_rgb, 
                    num_steps=num_steps,
                    ensemble_size=ensemble_size,
                    processing_res=processing_res,
                    no_color=no_color,
                    dtype=dtype
                )
                
                # è½¬æ¢å›OpenCVæ ¼å¼
                depth_frame = np.array(depth_result)
                depth_frame_bgr = cv2.cvtColor(depth_frame, cv2.COLOR_RGB2BGR)
                
                # å†™å…¥è¾“å‡ºè§†é¢‘
                out.write(depth_frame_bgr)
                processed_idx += 1
                
            frame_idx += 1
            
            # æ¯100å¸§æ£€æŸ¥æ˜¯å¦ç”¨æˆ·å–æ¶ˆ
            if frame_idx % 100 == 0 and progress.is_cancelled():
                break
                
    except Exception as e:
        raise RuntimeError(f"è§†é¢‘å¤„ç†å‡ºé”™: {str(e)}")
    finally:
        # é‡Šæ”¾èµ„æº
        cap.release()
        out.release()
        progress(1.0, desc="å¤„ç†å®Œæˆ")
        
    return output_path

# ==================== Gradioç•Œé¢é…ç½® ====================
# ç¤ºä¾‹æ–‡ä»¶é…ç½®
EXAMPLE_DIR = "examples"  # ç¤ºä¾‹å›¾ç‰‡å­˜æ”¾ç›®å½•
demo_samples = [
    [os.path.join(EXAMPLE_DIR, "img/dog.png"), 2, 4, -1, False],
]

# ç•Œé¢å¸ƒå±€
with gr.Blocks(title="DepthFM æ·±åº¦ä¼°è®¡æ¼”ç¤º", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¯ DepthFM å•ç›®æ·±åº¦ä¼°è®¡ç³»ç»Ÿ")
    gr.Markdown(" ä¸Šä¼ å›¾ç‰‡æˆ–è§†é¢‘ç”Ÿæˆæ·±åº¦å›¾ï¼Œæ”¯æŒå‚æ•°è°ƒèŠ‚å’Œå¤šç§è¾“å‡ºæ ¼å¼")
    
    with gr.Tabs():
        # === å›¾åƒå¤„ç†é€‰é¡¹å¡ ===
        with gr.TabItem("å›¾åƒæ·±åº¦ä¼°è®¡"):
            with gr.Row():
                # å·¦ä¾§æ§åˆ¶é¢æ¿
                with gr.Column(scale=1):
                    img_input = gr.Image(label="è¾“å…¥å›¾åƒ", type="numpy")
                    
                    with gr.Accordion("é«˜çº§å‚æ•°", open=False):
                        processing_res = gr.Slider(
                            minimum=64, maximum=2048, value=-1,
                            label="å¤„ç†åˆ†è¾¨ç‡ï¼ˆ-1=è‡ªåŠ¨ï¼‰", step=64
                        )
                        num_steps = gr.Slider(
                            minimum=1, maximum=10, value=2,
                            label="ODEæ±‚è§£æ­¥æ•°", step=1
                        )
                        ensemble_size = gr.Slider(
                            minimum=1, maximum=10, value=4,
                            label="é›†æˆæ¬¡æ•°", step=1
                        )
                        dtype = gr.Dropdown(
                            ["fp16", "fp32", "bf16"], value="fp16",
                            label="è®¡ç®—ç²¾åº¦"
                        )
                        no_color = gr.Checkbox(
                            label="ç°åº¦è¾“å‡º", info="å‹¾é€‰å¯ç”¨å•é€šé“æ·±åº¦å›¾"
                        )
                    
                    submit_btn = gr.Button("ğŸš€ å¼€å§‹è®¡ç®—", variant="primary")
                
                # å³ä¾§ç»“æœå±•ç¤º
                with gr.Column(scale=2):
                    img_output = gr.Image(label="æ·±åº¦å›¾ç»“æœ", type="pil")
            
            # ç¤ºä¾‹åŒºå—
            gr.Examples(
                examples=demo_samples,
                inputs=[img_input, num_steps, ensemble_size, processing_res, no_color],
                outputs=img_output,
                fn=process_image,
                cache_examples=False,  # ç¦ç”¨ç¼“å­˜é¿å…è·¯å¾„é—®é¢˜
                label="å¿«é€Ÿç¤ºä¾‹"
            )

            # æŒ‰é’®äº‹ä»¶ç»‘å®š
            submit_btn.click(
                fn=process_image,
                inputs=[img_input, num_steps, ensemble_size, processing_res, no_color, dtype],
                outputs=img_output
            )
        
        # === è§†é¢‘å¤„ç†é€‰é¡¹å¡ ===
        with gr.TabItem("è§†é¢‘æ·±åº¦ä¼°è®¡"):
            with gr.Row():
                # å·¦ä¾§æ§åˆ¶é¢æ¿
                with gr.Column(scale=1):
                    video_input = gr.Video(label="è¾“å…¥è§†é¢‘")
                    
                    with gr.Accordion("è§†é¢‘å¤„ç†å‚æ•°", open=True):
                        video_processing_res = gr.Slider(
                            minimum=128, maximum=1024, value=512,
                            label="å¤„ç†åˆ†è¾¨ç‡", step=64
                        )
                        video_frame_stride = gr.Slider(
                            minimum=1, maximum=10, value=1,
                            label="å¸§é‡‡æ ·æ­¥é•¿", step=1,
                            info="æ¯éš”å¤šå°‘å¸§å¤„ç†ä¸€å¸§ï¼ˆå€¼è¶Šå¤§å¤„ç†è¶Šå¿«ï¼Œä½†æµç•…åº¦é™ä½ï¼‰"
                        )
                    
                    with gr.Accordion("æ·±åº¦ä¼°è®¡å‚æ•°", open=False):
                        video_num_steps = gr.Slider(
                            minimum=1, maximum=10, value=2,
                            label="ODEæ±‚è§£æ­¥æ•°", step=1
                        )
                        video_ensemble_size = gr.Slider(
                            minimum=1, maximum=10, value=4,
                            label="é›†æˆæ¬¡æ•°", step=1
                        )
                        video_dtype = gr.Dropdown(
                            ["fp16", "fp32", "bf16"], value="fp16",
                            label="è®¡ç®—ç²¾åº¦"
                        )
                        video_no_color = gr.Checkbox(
                            label="ç°åº¦è¾“å‡º", info="å‹¾é€‰å¯ç”¨å•é€šé“æ·±åº¦å›¾"
                        )
                    
                    video_submit_btn = gr.Button("ğŸ¬ å¼€å§‹å¤„ç†è§†é¢‘", variant="primary")
                
                # å³ä¾§ç»“æœå±•ç¤º
                with gr.Column(scale=2):
                    video_output = gr.Video(label="æ·±åº¦è§†é¢‘ç»“æœ")
                    video_status = gr.Textbox(label="å¤„ç†çŠ¶æ€", value="ç­‰å¾…å¤„ç†...", interactive=False)
            
            # è§†é¢‘å¤„ç†äº‹ä»¶ç»‘å®š
            video_submit_btn.click(
                fn=process_video,
                inputs=[video_input, video_num_steps, video_ensemble_size, 
                       video_processing_res, video_no_color, video_dtype, video_frame_stride],
                outputs=video_output
            )

# ==================== å¯åŠ¨åº”ç”¨ ====================
if __name__ == "__main__":
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,  # ç”Ÿæˆå…¬å…±è®¿é—®é“¾æ¥
        allowed_paths=[os.path.abspath(EXAMPLE_DIR)]  # å…è®¸è®¿é—®ç¤ºä¾‹ç›®å½•
    )



